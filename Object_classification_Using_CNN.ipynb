{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Classification Using CNN -Acc 0.8235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Introduction\n",
    "    2. Data preparation\n",
    "        2.1 Load data\n",
    "        2.2 Reshape\n",
    "        2.3 Label encoding\n",
    "        2.4 Split training and valdiation set\n",
    "    3. CNN\n",
    "        3.1 Define the model\n",
    "        3.2 Set the optimizer\n",
    "    4. Evaluate the model\n",
    "        4.1 Training and validation \n",
    "    5. Prediction \n",
    "        5.1 Predict results\n",
    "    6. Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1.Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 5 layers Sequential Convolutional Neural Network for object classification trained on the dataset downloaded from IKEA site (https://www.ikea.com/). I choosed to build it with keras API (Tensorflow backend) which is very intuitive. Firstly, I will prepare the data then i will focus on the CNN modeling and evaluation.\n",
    "\n",
    "I achieved 82.35% of accuracy with this CNN trained.\n",
    "\n",
    "\n",
    "This Notebook follows three main parts:\n",
    "\n",
    "The data preparation\n",
    "The CNN modeling and evaluation\n",
    "The results prediction\n",
    "The Confusion matrix generation\n",
    "\n",
    "IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "merx7lWMWN0v",
    "outputId": "b3990cfc-aacb-4a19-c49b-888c1f710114"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Input,UpSampling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "from keras.layers import merge\n",
    "from keras.layers import Reshape\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DATADIR=\"/content/drive/My Drive/dataset\"\n",
    "CATEGORIES=[\"2.dining_chairs\",\"3.beds\",\"4.sofa\",\"5.storage_furniture\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The dataset will be read in above cell from the path DATADIR. There are four classes in the dataset.             Those are:  \n",
    "        1.Dining chairs\n",
    "        2.Beds\n",
    "        3.Sofa\n",
    "        4.Storage_furniture\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KjIk8BWbWN1G",
    "outputId": "a64d3f02-b033-4dd1-9b83-f6a16cf88ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421\n"
     ]
    }
   ],
   "source": [
    "training_data=[]\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path=os.path.join(DATADIR,category)\n",
    "        class_num=CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array=cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)\n",
    "                new_array=cv2.resize(img_array,(400,400))\n",
    "                training_data.append([new_array,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "create_training_data()\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffled the data randomly and I saved the features and lables to the arrays x and y respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YTke9xj8WN1S"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(training_data)\n",
    "x=[]\n",
    "y=[]\n",
    "for features,label in training_data:\n",
    "    x.append(features)\n",
    "    y.append(label)\n",
    "x=np.array(x).reshape(-1,400,400,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Splitting the dataset into training, testing and validation sets.\n",
    "    \n",
    "    train set :- 60 % of dataset\n",
    "    test set :- 40 % of dataset\n",
    "    validation set :- 50 % of test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "202b7WN4WN1c",
    "outputId": "7775403d-4c2d-4e90-8222-9e588f4f4424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (252, 400, 400, 1) 252\n",
      "Testing data shape :  (169, 400, 400, 1) 169\n",
      "Total number of outputs :  4\n",
      "Output classes :  [0 1 2 3]\n",
      "252\n",
      "169\n",
      "Original label :  3\n",
      "After conversion to categorical ( one-hot ) :  [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "train_images, test_images, train_labels, test_labels = train_test_split(x, y, test_size=0.4, random_state=42)\n",
    "print('Training data shape : ', train_images.shape, len(train_labels))\n",
    "print('Testing data shape : ', test_images.shape, len(test_labels))\n",
    "classes = np.unique(train_labels)\n",
    "#classes=np.append(classes,0)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)\n",
    "train_data = train_images.astype('float32')\n",
    "test_data = test_images.astype('float32')\n",
    "train_data /= 255\n",
    "test_data /= 255\n",
    "print(len(train_labels))\n",
    "print(len(test_labels))\n",
    "train_labels_one_hot = to_categorical(train_labels)\n",
    "test_labels_one_hot = to_categorical(test_labels)\n",
    "print('Original label : ', train_labels[0])\n",
    "print('After conversion to categorical ( one-hot ) : ', train_labels_one_hot[0])\n",
    "validation_images, test_images, validation_labels, test_labels = train_test_split(test_data,test_labels_one_hot, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Define the model\n",
    "\n",
    "I used the Keras Sequential API, where you have just to add one layer at a time, starting from the input.\n",
    "\n",
    "Here I used 5 convolution layers. The first is the convolutional (Conv2D) layer. It is like a set of learnable filters. I choosed to set 32 filters for the first three conv2D layers and 64 filters for the next ones. The last conv2D layer have 128 filters. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image.\n",
    "\n",
    "The second important layer in CNN is the pooling (MaxPool2D) layer. This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the\n",
    "pooling dimension is high, more the downsampling is important.\n",
    "\n",
    "Combining convolutional and pooling layers, CNN are able to combine local features and learn more global features of the image.\n",
    "\n",
    "Dropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This technique also improves generalization and reduces the overfitting.\n",
    "'Relu' is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n",
    "The Flatten layer is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional/maxpool layers. It combines all the found local features of the previous convolutional layers.\n",
    "In the end I used the features in two fully-connected (Dense) layers which is just artificial an neural networks (ANN) classifier. In the last layer(Dense(10,activation=\"softmax\")) the net outputs distribution of probability of each class.\n",
    "    \n",
    "Once our layers are added to the model, we need to set up a score function, a loss function and an optimisation algorithm.\n",
    "I used a specific form for categorical classifications (>2 classes) called the \"categorical_crossentropy\" as a loss function.\n",
    "\n",
    "I choosed Adam as optimizer, it is a Straightforward to implement,Computationally efficient, Little memory requirements and Well suited for problems that are large in terms of data and/or parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 888
    },
    "colab_type": "code",
    "id": "-tvzaCGDWN1m",
    "outputId": "a18094df-9abe-44f9-a4af-973ec66a3ce9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 398, 398, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 199, 199, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 199, 199, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 197, 197, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 98, 98, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 98, 98, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 56448)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               7225472   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 7,337,156\n",
      "Trainable params: 7,337,156\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn3 = Sequential()\n",
    "cnn3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(400, 400, 1)))\n",
    "cnn3.add(MaxPooling2D((2, 2)))\n",
    "cnn3.add(Dropout(0.25))\n",
    "\n",
    "cnn3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(400, 400, 1)))\n",
    "cnn3.add(MaxPooling2D((2, 2)))\n",
    "cnn3.add(Dropout(0.25))\n",
    "cnn3.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(400, 400, 1)))\n",
    "cnn3.add(MaxPooling2D((2, 2)))\n",
    "cnn3.add(Dropout(0.25))\n",
    "\n",
    "cnn3.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "cnn3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn3.add(Dropout(0.25))\n",
    "\n",
    "cnn3.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "cnn3.add(Dropout(0.4))\n",
    "\n",
    "cnn3.add(Flatten())\n",
    "\n",
    "cnn3.add(Dense(128, activation='relu'))\n",
    "cnn3.add(Dropout(0.5))\n",
    "cnn3.add(Dense(4, activation='softmax'))\n",
    "\n",
    "cnn3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "cnn3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I train the model for a given number of epochs (iterations on a dataset) with batch size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    },
    "colab_type": "code",
    "id": "Y-RLJANWWN1v",
    "outputId": "13012673-069c-41a2-e379-9c3c332f2136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 252 samples, validate on 84 samples\n",
      "Epoch 1/20\n",
      "252/252 [==============================] - 7s 29ms/step - loss: 1.4083 - acc: 0.3690 - val_loss: 1.3298 - val_acc: 0.4405\n",
      "Epoch 2/20\n",
      "252/252 [==============================] - 3s 14ms/step - loss: 1.3588 - acc: 0.4206 - val_loss: 1.3603 - val_acc: 0.4405\n",
      "Epoch 3/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 1.2752 - acc: 0.4881 - val_loss: 1.2125 - val_acc: 0.5119\n",
      "Epoch 4/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 1.0283 - acc: 0.5873 - val_loss: 1.1354 - val_acc: 0.5714\n",
      "Epoch 5/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.9040 - acc: 0.6429 - val_loss: 1.1060 - val_acc: 0.5357\n",
      "Epoch 6/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.7562 - acc: 0.6944 - val_loss: 0.9518 - val_acc: 0.6190\n",
      "Epoch 7/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.6817 - acc: 0.7500 - val_loss: 1.0027 - val_acc: 0.6310\n",
      "Epoch 8/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.5068 - acc: 0.8016 - val_loss: 1.2049 - val_acc: 0.7143\n",
      "Epoch 9/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.4524 - acc: 0.8016 - val_loss: 1.1884 - val_acc: 0.6905\n",
      "Epoch 10/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.2760 - acc: 0.8889 - val_loss: 1.5371 - val_acc: 0.7500\n",
      "Epoch 11/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.3337 - acc: 0.9127 - val_loss: 1.3429 - val_acc: 0.7262\n",
      "Epoch 12/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.3440 - acc: 0.9048 - val_loss: 1.2334 - val_acc: 0.7262\n",
      "Epoch 13/20\n",
      "252/252 [==============================] - 4s 15ms/step - loss: 0.2972 - acc: 0.9206 - val_loss: 1.2545 - val_acc: 0.6905\n",
      "Epoch 14/20\n",
      "252/252 [==============================] - 4s 15ms/step - loss: 0.1670 - acc: 0.9484 - val_loss: 1.3625 - val_acc: 0.7262\n",
      "Epoch 15/20\n",
      "252/252 [==============================] - 4s 15ms/step - loss: 0.2396 - acc: 0.9643 - val_loss: 1.3434 - val_acc: 0.7143\n",
      "Epoch 16/20\n",
      "252/252 [==============================] - 4s 15ms/step - loss: 0.2181 - acc: 0.9365 - val_loss: 1.6184 - val_acc: 0.7262\n",
      "Epoch 17/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.1309 - acc: 0.9444 - val_loss: 1.5737 - val_acc: 0.6905\n",
      "Epoch 18/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.2571 - acc: 0.9603 - val_loss: 1.9618 - val_acc: 0.7262\n",
      "Epoch 19/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.0932 - acc: 0.9802 - val_loss: 1.6436 - val_acc: 0.7381\n",
      "Epoch 20/20\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 0.0125 - acc: 0.9960 - val_loss: 2.0455 - val_acc: 0.7619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa7a1724090>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn3.fit(train_data,train_labels_one_hot,epochs=20,batch_size=1,verbose=1,validation_data=(validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I evaluate on testing data:\n",
    "    \n",
    "    Got 82.35 % of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5klK_65DfzhQ",
    "outputId": "acea6a83-eb56-477a-cd10-4893d47f81f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.443966994566076, 0.8235294131671681]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn3.evaluate(test_images, test_labels,verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model saved in my drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lJCow7wvWN18"
   },
   "outputs": [],
   "source": [
    "cnn3.save_weights('/content/drive/My Drive/dataset/model2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 5.Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with the saved model I predict the classification of test images by using predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "TLqXy7ImZ-PL"
   },
   "outputs": [],
   "source": [
    "predicted=cnn3.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6.Confusion Matrix Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for obtaining lables in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "cPENBBVDaQ4O"
   },
   "outputs": [],
   "source": [
    "import numpy \n",
    "x=0\n",
    "y=[]\n",
    "# Function to get max element \n",
    "def maxelement(arr): \n",
    "\t\n",
    "\t# get number of rows and columns \n",
    "\tno_of_rows = len(arr) \n",
    "\tno_of_column = len(arr[0]) \n",
    "\t\n",
    "\tfor i in range(no_of_rows): \n",
    "\t\t\n",
    "\t\t# Initialize max1 to 0 at beginning \n",
    "\t\t# of finding max element of each row \n",
    "\t\tmax1 = 0\n",
    "\t\tfor j in range(no_of_column): \n",
    "\t\t\tif arr[i][j] > max1 : \n",
    "\t\t\t\tmax1 = arr[i][j] \n",
    "\t\t\t\tx=j\n",
    "\t\t# print maximum element of each row  \n",
    "\t\ty.append(x)\n",
    "# Driver Code \n",
    "\treturn(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I applied the above function for predicted and actual lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "YzQ_1FSeaURo",
    "outputId": "4f2c74f7-c815-4d5a-b884-5c18062abcf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 2, 1, 0, 0, 2, 1, 1, 2, 0, 0, 3, 2, 3, 0, 0, 3, 3, 2, 0, 2, 3, 3, 1, 3, 1, 3, 0, 3, 3, 2, 2, 2, 3, 2, 3, 1, 3, 0, 0, 3, 2, 3, 2, 1, 3, 2, 2, 1, 0, 2, 2, 2, 0, 2, 1, 0, 3, 1, 2, 3, 2, 2, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 1, 3, 0, 3, 2, 0, 3, 2, 0, 2, 0, 2, 3, 2, 1, 0, 0, 3, 3, 1, 2, 0, 0, 3, 3, 3, 0, 0, 3, 3, 2, 3, 2, 3, 3, 1, 3, 3, 2, 1, 3, 3, 2, 1, 2, 3, 2, 3, 1, 3, 0, 0, 3, 2, 3, 2, 1, 3, 3, 2, 1, 0, 2, 3, 2, 1, 2, 1, 2, 3, 1, 2, 2, 2, 3, 2, 2, 3, 1, 2, 0, 2, 0, 0, 1, 1, 3, 0, 3, 2, 0, 3, 2, 0, 2]\n",
      "[0, 2, 3, 2, 1, 0, 0, 2, 1, 1, 2, 0, 0, 3, 2, 3, 0, 0, 3, 3, 2, 0, 2, 3, 3, 1, 3, 1, 3, 0, 3, 3, 2, 2, 2, 3, 2, 3, 1, 3, 0, 0, 3, 2, 3, 2, 1, 3, 2, 2, 1, 0, 2, 2, 2, 0, 2, 1, 0, 3, 1, 2, 3, 2, 2, 2, 2, 3, 1, 2, 0, 2, 0, 0, 2, 1, 3, 0, 3, 2, 0, 3, 2, 0, 2, 0, 2, 3, 2, 1, 0, 0, 3, 3, 1, 2, 0, 0, 3, 3, 3, 0, 0, 3, 3, 2, 3, 2, 3, 3, 1, 3, 3, 2, 1, 3, 3, 2, 1, 2, 3, 2, 3, 1, 3, 0, 0, 3, 2, 3, 2, 1, 3, 3, 2, 1, 0, 2, 3, 2, 1, 2, 1, 2, 3, 1, 2, 2, 2, 3, 2, 2, 3, 1, 2, 0, 2, 0, 0, 1, 1, 3, 0, 3, 2, 0, 3, 2, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "a=maxelement(predicted)\n",
    "b=maxelement(test_labels)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gave the above outputs to the input of confusion matrix function, The result is the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "iy45udQEa5I7",
    "outputId": "c630a35d-1e6a-4fbf-9dfa-0aadbc2a01f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[36  0  0  0]\n",
      " [ 0 26  0  0]\n",
      " [ 0  0 56  0]\n",
      " [ 0  0  0 52]]\n",
      "Accuracy Score : 1.0\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        36\n",
      "           1       1.00      1.00      1.00        26\n",
      "           2       1.00      1.00      1.00        56\n",
      "           3       1.00      1.00      1.00        52\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       170\n",
      "   macro avg       1.00      1.00      1.00       170\n",
      "weighted avg       1.00      1.00      1.00       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Python script for confusion matrix creation. \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "results = confusion_matrix(b, a) \n",
    "print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(b, a))\n",
    "print('Report : ')\n",
    "print(classification_report(b, a)) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "obj_class_2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
